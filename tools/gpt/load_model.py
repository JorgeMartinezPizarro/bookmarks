from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from flask import Flask, request

# 游댳 Carga del modelo optimizada para CPU
model_name = "EleutherAI/gpt-j-6B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,
    device_map={"": "cpu"}
)

# 游댳 Cargamos el tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # 游댳 Define token de padding

app = Flask(__name__)

@app.route("/gpt", methods=["POST"])
def chat():
    data = request.json
    messages = data.get("messages", [])
    if not messages:
        return "Error: No messages provided", 400

    # 游댳 Solo tomamos la 칰ltima pregunta
    ultima_pregunta = messages[-1]["content"] if messages else ""

    # 游댳 Reformulamos el prompt para enfocarlo en resultados matem치ticos
    prompt = f"Pregunta: {ultima_pregunta}\nRespuesta matem치tica precisa y breve:"

    # 游댳 Tokenizaci칩n optimizada
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

    # 游댳 Inferencia optimizada sin gradientes para mejorar velocidad
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"], 
            attention_mask=inputs["attention_mask"],  
            max_new_tokens=50,  # 游댳 Genera respuestas m치s r치pidas y concretas
            num_return_sequences=1,
            temperature=0.3,  # 游댳 Prioriza precisi칩n sobre creatividad
            top_k=30,  
            top_p=0.7,  # 游댳 Reduce la variabilidad de respuestas
            repetition_penalty=1.2,  # 游댳 Evita que repita frases sin sentido
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            do_sample=False  
        )

    # 游댳 Procesamos la respuesta para eliminar el prompt y limpiar el texto
    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    # 游댳 Eliminamos la pregunta si el modelo la repite
    response = response.replace(ultima_pregunta, "").strip()
    response = response.replace("Pregunta:", "").strip()
    response = response.replace("Respuesta matem치tica precisa y breve:", "").strip()

    # 游댳 Eliminamos cualquier l칤nea en blanco que haya quedado
    response = "\n".join([line.strip() for line in response.split("\n") if line.strip()])

    # 游댳 Si la respuesta no termina en punto, lo agregamos
    if response and response[-1] not in ".!?":
        response += "."

    # 游댳 Devuelve texto puro, sin JSON
    print(response)  # 游댳 Se ver치 en los logs de Docker
    return response, 200, {"Content-Type": "text/plain"}

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
